{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"As we create and consume increasing amounts of digital data, we rely on algorithms to help make sense of it all. From surfacing content in video streaming services to setting bail for a defendant in a criminal trial, algorithms sift through our personal data, websites, and databases to inform decisions every day—with potentially serious consequences. Though they appear neutral, algorithms can reflect and amplify racial, gender, or other biases from unrepresentative training data. What tools can we use to ensure that society reaps the benefits of algorithms without worsening existing inequalities? How should fairness models and ethics be integrated into the design and execution of these models?\n",
    "\n",
    "On January 14, the Center for Technology Innovation at Brookings will host Michael Kearns and Aaron Roth of the University of Pennsylvania to discuss their book, “The Ethical Algorithm.” \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moderator = Nicole Turner Lee  \n",
    "Panelists = Michael Kearns, Aaron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to pose mathematical definitions of the concepts of fairness, and what are the consequences of that (societally, and computationally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you mean by precise definitions of fairness, accuracy. \n",
    "\n",
    "Fairness = a group you want to protect, and what you want to protect them from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matters for predictive models in lending. \n",
    "Defining fairness as the differential between false negative rates between the majority and minority group\n",
    "\n",
    "What's the actionn that causes harm? The harm shouldn't be disportionately concentrated in one group. \n",
    "\n",
    "Being cautious of what you use as proxies. \n",
    "\n",
    "Addressing data science and machine learning to enforce fairness consideration:\n",
    "\n",
    "- solve the \"constrained optimization problem\". put a constraint that the \"false rejection rate\" between segments of population. \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
